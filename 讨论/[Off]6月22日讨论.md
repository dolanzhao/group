# [线下]6月22日讨论
## 随机梯度下降在神经网络中的应用
* 权重更新公式

![equation1](https://latex.codecogs.com/svg.latex?w_{jk}^{l}\longrightarrow{w_{jk}^{l}}-\eta\frac{\partial{C}}{\partial{w_{jk}^{l}}})

![equation1](https://latex.codecogs.com/svg.latex?b_{j}^{l}\longrightarrow{b_{j}^{l}}-\eta\frac{\partial{C}}{\partial{b_{j}^{l}}})

算法重点是求得![equation1](https://latex.codecogs.com/svg.latex?\frac{\partial{C}}{\partial{w_{jk}^{l}}})和![equation1](https://latex.codecogs.com/svg.latex?\frac{\partial{C}}{\partial{b_{j}^{l}}})
## 基于代价定义的误差的反向传播过程
* 误差定义
定义第$l$层第$j$个神经元的误差为

![equation1](https://latex.codecogs.com/svg.latex?\delta_j^l=\frac{\partial{C}}{\partial{z_j^l}})

这个误差是中间变量为了方便计算权重和偏置的偏导数
* 四个基本公式

![equation1](https://latex.codecogs.com/svg.latex?\delta^L=\bigtriangledown_a{C}\odot\sigma^{'}(z^L))

![equation1](https://latex.codecogs.com/svg.latex?\delta^l=\((w^{l&plus;1})^T\delta^{l&plus;1}\)}\odot\sigma^{'}(z^l))

![equation1](https://latex.codecogs.com/svg.latex?\frac{\partial{C}}{\partial{b_{j}^{l}}}=\delta^l_j)

![equation1](https://latex.codecogs.com/svg.latex?\frac{\partial{C}}{\partial{w_{jk}^{l}}}=a^{l-1}_k\delta^l_j)

* 公式证明
链式法则参考"神经⽹络与深度学习"
